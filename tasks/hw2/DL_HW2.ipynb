{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Course\n",
    "\n",
    "### Homework 2\n",
    "\n",
    "#### Anastasiia Kasprova\n",
    "\n",
    "    Link to github: https://github.com/kasprova/DL_UCU/tree/master/tasks/hw2\n",
    "    Link to colab: https://colab.research.google.com/drive/1dYLb2ZRpyKemuOmglM5rKUoEhXJftWa2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Custom Functions Implementation (simple_conv_net_func.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_mse(x, y):\n",
    "    x_vec = x.view(1, -1).squeeze()\n",
    "    y_vec = y.view(1, -1).squeeze()\n",
    "    return torch.mean(torch.pow((x_vec - y_vec), 2)).item()\n",
    "  \n",
    "\n",
    "def conv2d_scalar(x_in, conv_weight, conv_bias, device):\n",
    "    \n",
    "    #read dimentionas of input tensor and weights\n",
    "    batch_size, n_channels_in, height_in, width_in = x_in.shape\n",
    "    n_channels_out, n_channels_in, kernel_size, kernel_size = conv_weight.shape\n",
    "    \n",
    "    #calculate the dimentions of output tensor\n",
    "    height_out = height_in - kernel_size + 1\n",
    "    width_out = width_in - kernel_size + 1\n",
    "    \n",
    "    #move to device\n",
    "    x_in = x_in.to(device)\n",
    "    conv_weight = conv_weight.to(device)\n",
    "    conv_bias = conv_bias.to(device)\n",
    "    \n",
    "    #intiale output tensor of the correct size\n",
    "    z = torch.zeros([batch_size,n_channels_out,height_out,width_out]).to(device)\n",
    "    z.requires_grad = True\n",
    "    \n",
    "    #fulfill z based on scalar representation\n",
    "    for n in range(batch_size):\n",
    "        for c_out in range(n_channels_out):\n",
    "            for c_in in range(n_channels_in):\n",
    "                for m in range(height_out):\n",
    "                    for l in range(width_out):\n",
    "                        z[n,c_out,m,l] = (x_in[n,c_in,m:m+kernel_size,l:l+kernel_size]*conv_weight[c_out,c_in]).sum() + conv_bias[c_out]\n",
    "                                                                                                                                                                                                                                                                                                                                                          \n",
    "    return z\n",
    "\n",
    "  \n",
    "def im2col(X, kernel_size, device, stride = 1):\n",
    "  \n",
    "    #read dimentions of input tensor - 3-dimentional\n",
    "    C_in, S_in, S_in = X.shape\n",
    "\n",
    "    #calculate size_out\n",
    "    S_out = (S_in - kernel_size)//stride + 1\n",
    "    \n",
    "    #move to device\n",
    "    X = X.to(device)\n",
    "    \n",
    "    #intiale output tensor of the correct size\n",
    "    X_cols = torch.zeros([S_out*S_out, kernel_size*kernel_size]).to(device)\n",
    "    X_cols.requires_grad = True\n",
    "    \n",
    "    for i in range(S_out):\n",
    "        for j in range(S_out):\n",
    "            X_cols[i*S_out+j] = X[0][i: i + kernel_size, j: j + kernel_size].contiguous().view(1, -1)\n",
    "    \n",
    "    return X_cols.t() # [K*K x S_out*S_out]\n",
    "\n",
    "  \n",
    "def conv_weight2rows(conv_weight):\n",
    "    \n",
    "    ##read dimentions of input tensor\n",
    "    C_out = conv_weight.shape[0]\n",
    "    kernel_size = conv_weight.shape[2]\n",
    "    \n",
    "    #resize \n",
    "    conv_weight_rows = conv_weight.view(C_out,kernel_size*kernel_size).contiguous()\n",
    "    \n",
    "    return conv_weight_rows # [C_out x K*K]\n",
    "  \n",
    "\n",
    "def conv2d_vector(x_in, conv_weight, conv_bias, device):\n",
    "    #read dimentionas of input tensor and weights\n",
    "    batch_size, C_in, S_in, S_in = x_in.shape\n",
    "    C_out, C_in, kernel_size, kernel_size = conv_weight.shape\n",
    "    \n",
    "    #calculate the dimentions of output tensor\n",
    "    S_out = S_in - kernel_size + 1\n",
    "    \n",
    "    #move to device\n",
    "    x_in = x_in.to(device)\n",
    "    conv_weight = conv_weight.to(device)\n",
    "    conv_bias = conv_bias.to(device)\n",
    "    \n",
    "    #intiale output tensor of the correct size\n",
    "    z = torch.zeros([batch_size,C_out,S_out,S_out]).to(device)\n",
    "    z.requires_grad = True\n",
    "    \n",
    "    #transformation of conv_weight\n",
    "    conv_weight_rows = conv_weight2rows(conv_weight)\n",
    "    \n",
    "    for n in range(batch_size):\n",
    "        #WconvX+b, dim(WconvX+b)=[C_out x S_out*S_out], reshape [C_out x S_out x S_out]\n",
    "        z[n] = (conv_weight_rows.matmul(im2col(x_in[n], kernel_size, device, stride=1)) + conv_bias.view(-1,1)).view(C_out,S_out,S_out)\n",
    "    \n",
    "    return z\n",
    "\n",
    "  \n",
    "def pool2d_scalar(a, device, stride = 2):\n",
    "    \n",
    "    #read dimentionas of input tensor\n",
    "    batch_size, n_channels_in, height_in, width_in = a.shape\n",
    "    pooling_size = 2\n",
    "    \n",
    "    #calculate the dimentions of output tensor\n",
    "    height_out = (height_in-pooling_size)//stride + 1\n",
    "    width_out = (width_in-pooling_size)//stride + 1\n",
    "    n_channels_out = n_channels_in\n",
    "    \n",
    "    #move to device\n",
    "    a = a.to(device)\n",
    "    \n",
    "    #intiale an output tensor of the correct size\n",
    "    z = torch.zeros([batch_size,n_channels_out,height_out,width_out]).to(device)\n",
    "    z.requires_grad = True\n",
    "    \n",
    "    #fulfill z based on scalar representation\n",
    "    for n in range(batch_size):\n",
    "        for c_out in range(n_channels_out):\n",
    "            for i in range(height_out):\n",
    "                for j in range(width_out):\n",
    "                    z[n,c_out,i,j] = a[n,c_out,2*i:2*i+2,2*j:2*j+2].max()\n",
    "    \n",
    "    return z\n",
    "   \n",
    "def pool2d_vector(a, device, stride = 2):\n",
    "    \n",
    "    #read dimentionas of input tensor\n",
    "    batch_size, C_in, S_in, S_in = a.shape\n",
    "    pooling_size = 2\n",
    "    stride = 2\n",
    "    \n",
    "    #calculate the dimentions of output tensor\n",
    "    S_out = (S_in - pooling_size)//stride + 1 \n",
    "    C_out = C_in\n",
    "    \n",
    "    #move to device\n",
    "    a = a.to(device)\n",
    "    \n",
    "    #intiale an output tensor of the correct size\n",
    "    z = torch.zeros([batch_size,C_out,S_out,S_out]).to(device)\n",
    "    z.requires_grad = True\n",
    "    \n",
    "    for n in range(batch_size):\n",
    "        z[n] = im2col(a[n], pooling_size, device, stride=2).max(dim=0).values.view(-1, S_out, S_out)\n",
    "    return z \n",
    "  \n",
    "\n",
    "def relu_scalar(a, device):\n",
    "  \n",
    "    #read dimentionas of input matrix\n",
    "    batch_size, n_inputs = a.shape\n",
    "    \n",
    "    #move to device\n",
    "    a = a.to(device)\n",
    "    \n",
    "    #intiale an output matrix of the correct size\n",
    "    z = torch.zeros([batch_size, n_inputs]).to(device)\n",
    "    z.requires_grad = True\n",
    "    \n",
    "    for n in range(batch_size):\n",
    "        for i in range(n_inputs):\n",
    "            if a[n,i]<0:\n",
    "                z[n,i]=0\n",
    "            else:\n",
    "                z[n,i]=a[n,i]\n",
    "                \n",
    "    return z\n",
    "  \n",
    "\n",
    "def relu_vector(a, device):\n",
    "    #move to device\n",
    "    a = a.to(device)\n",
    "    \n",
    "    #clone input tensor\n",
    "    z = a.clone().to(device)\n",
    "    \n",
    "    #elements < 0 replace with 0\n",
    "    z[z<0] = 0\n",
    "    \n",
    "    return z\n",
    "\n",
    "  \n",
    "def reshape_scalar(a, device):\n",
    "    \n",
    "    #read dimentionas of input tensor\n",
    "    batch_size, n_channels_in, height_in, width_in = a.shape\n",
    "    \n",
    "    #calculate the dimentions of output tensor\n",
    "    n_outputs = n_channels_in * height_in * width_in\n",
    "    \n",
    "    #move to device\n",
    "    a = a.to(device)\n",
    "    \n",
    "    #intiale an output matrix of the correct size\n",
    "    z = torch.zeros([batch_size, n_outputs]).to(device)\n",
    "    z.requires_grad = True\n",
    "    \n",
    "    for n in range(batch_size):\n",
    "        for c_in in range(n_channels_in):\n",
    "            for m in range(height_in):\n",
    "                for l in range(width_in):\n",
    "                    z[n,c_in*height_in*width_in+m*height_in+l] = a[n,c_in,m,l]\n",
    "    \n",
    "    return z\n",
    "  \n",
    "def reshape_vector(a, device):\n",
    "    \n",
    "    batch_size = a.shape[0]\n",
    "    \n",
    "    #move to device\n",
    "    a = a.to(device)\n",
    "    \n",
    "    z = a.clone().view(batch_size,-1).requires_grad_(True)\n",
    "    \n",
    "    return z\n",
    "  \n",
    "def fc_layer_scalar(a, weight, bias, device):\n",
    "    \n",
    "    #read dimentionas of input matrix\n",
    "    batch_size, n_inputs = a.shape\n",
    "    n_outputs = bias.shape[0]\n",
    "    \n",
    "    #move to device\n",
    "    a = a.to(device)\n",
    "    weight = weight.to(device)\n",
    "    bias = bias.to(device)\n",
    "    \n",
    "    #intiale an output matrix of the correct size\n",
    "    z = torch.zeros([batch_size, n_outputs]).to(device)\n",
    "    z.requires_grad = True\n",
    "    \n",
    "    for n in range(batch_size):\n",
    "        for j in range(n_outputs):\n",
    "            z[n,j] = bias[j]\n",
    "            for i in range(n_inputs):\n",
    "                z[n,j] += weight[j,i]*a[n,i]\n",
    "                \n",
    "    return z\n",
    "  \n",
    "    \n",
    "def fc_layer_vector(a, weight, bias, device):\n",
    "    \n",
    "    z = (a.matmul(weight.t())+ bias).clone().requires_grad_(True)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Check bellow functions (forward pass) using MSE metric whether their outputs are exactly the same as in Pytorch framework:\n",
    "    conv2d_scalar\n",
    "    pool2d_scalar\n",
    "    relu_scalar\n",
    "    fc_layer_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        self.device = device\n",
    "        self.conv_layer = nn.Conv2d(in_channels=1,\n",
    "                                    out_channels=20,\n",
    "                                    kernel_size=5,\n",
    "                                    stride=1,\n",
    "                                    padding=0,\n",
    "                                    dilation=1,\n",
    "                                    groups=1,\n",
    "                                    bias=True)\n",
    "\n",
    "        self.fc_layer1 = nn.Linear(in_features=20 * 12 * 12, out_features=500)\n",
    "        self.fc_layer2 = nn.Linear(in_features=500, out_features=10)\n",
    "        self.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments: Since training with MNIST data crashes the os when using 'cpu', the following testing will be done with a toy generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(17)\n",
    "np.random.seed(17)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleConvNet(\n",
       "  (conv_layer): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc_layer1): Linear(in_features=2880, out_features=500, bias=True)\n",
       "  (fc_layer2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization\n",
    "model = SimpleConvNet(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv_layer\n",
    "data = torch.rand([1,1,28,28])\n",
    "w_conv = torch.rand([20,1,5,5])\n",
    "b_conv = torch.rand([20])\n",
    "\n",
    "#fc1_layer\n",
    "w_fc1 = torch.rand([500, 2880])\n",
    "b_fc1 = torch.rand([500])\n",
    "\n",
    "#fc2_layer\n",
    "w_fc2 = torch.rand([10, 500])\n",
    "b_fc2 = torch.rand([10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conv2d_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_conv2d_torch = model.conv_layer(data)\n",
    "z_conv2d_scalar = conv2d_scalar(data, model.conv_layer.weight, model.conv_layer.bias, device)\n",
    "\n",
    "mse_conv2d_scalar = diff_mse(z_conv2d_torch,z_conv2d_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE difference between conv2d_scalar output and conv_layer torch native:  1.0867831877549067e-15\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE difference between conv2d_scalar output and conv_layer torch native: \", mse_conv2d_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pool2d_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pool2d_torch = F.max_pool2d(z_conv2d_torch, 2, 2)\n",
    "z_pool2d_scalar = pool2d_scalar(z_conv2d_scalar, device)\n",
    "\n",
    "mse_pool2d_scalar = diff_mse(z_pool2d_torch,z_pool2d_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE difference between pool2d_scalar output and max_pool2d torch native:  1.0151745988775545e-15\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE difference between pool2d_scalar output and max_pool2d torch native: \", mse_pool2d_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reshape_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pool2d_reshape_torch = z_pool2d_torch.view(-1, 20 * 12 * 12)\n",
    "z_pool2d_reshape_scalar = reshape_scalar(z_pool2d_scalar, device)\n",
    "\n",
    "mse_reshape_scalar = diff_mse(z_pool2d_reshape_torch,z_pool2d_reshape_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE difference between reshape_scalar output and torch.view(-1, 20 * 12 * 12):  1.0151745988775545e-15\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE difference between reshape_scalar output and torch.view(-1, 20 * 12 * 12): \", mse_reshape_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fc_layer_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_fc1_torch = model.fc_layer1(z_pool2d_reshape_torch)\n",
    "z_fc1_scalar = fc_layer_scalar(z_pool2d_reshape_scalar, model.fc_layer1.weight, model.fc_layer1.bias, device)\n",
    "\n",
    "mse_fc1_scalar = diff_mse(z_fc1_torch,z_fc1_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE difference between fc_layer_scalar output and fc_layer1 torch native:  4.632291390207176e-14\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE difference between fc_layer_scalar output and fc_layer1 torch native: \", mse_fc1_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compare the performance of  scalar and vector implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0195803642273 sec\n"
     ]
    }
   ],
   "source": [
    "start_scalar = time.time()\n",
    "#scalar\n",
    "z_conv = conv2d_scalar(data, w_conv, b_conv, device)\n",
    "z_pool = pool2d_scalar(z_conv, device)\n",
    "z_pool_reshaped = reshape_scalar(z_pool, device)\n",
    "z_fc1 = fc_layer_scalar(z_pool_reshaped, w_fc1, b_fc1, device)\n",
    "z_relu = relu_scalar(z_fc1, device)\n",
    "z_fc2 = fc_layer_scalar(z_relu, w_fc2, b_fc2, device)\n",
    "y = F.softmax(z_fc2, dim=1)\n",
    "end_scalar = time.time()\n",
    "\n",
    "print(end_scalar-start_scalar, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09873318672180176 sec\n"
     ]
    }
   ],
   "source": [
    "start_vector = time.time()\n",
    "#vector\n",
    "z_conv = conv2d_vector(data, w_conv, b_conv, device)\n",
    "z_pool = pool2d_vector(z_conv, device)\n",
    "z_pool_reshaped = reshape_vector(z_pool, device)\n",
    "z_fc1 = fc_layer_vector(z_pool_reshaped, w_fc1, b_fc1, device)\n",
    "z_relu = relu_vector(z_fc1, device)\n",
    "z_fc2 = fc_layer_vector(z_relu, w_fc2, b_fc2, device)\n",
    "y = F.softmax(z_fc2, dim=1)\n",
    "\n",
    "end_vector = time.time()\n",
    "\n",
    "print(end_vector-start_vector, \"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments: Vector implementation performs in more than 100 times faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
