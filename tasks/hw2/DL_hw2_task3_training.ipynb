{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_hw2_task3_training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXS2_U9xzmTS",
        "colab_type": "text"
      },
      "source": [
        "## Deep Learning Course\n",
        "\n",
        "### Homework 2\n",
        "\n",
        "### Task 3:\n",
        "    Train your network for 20 epochs, report the achieved accuracy on MNIST test data. Measure and report the time on one epoch for scalar and vector variants.\n",
        "\n",
        "#### Anastasiia Kasprova\n",
        "\n",
        "    Link to github: https://github.com/kasprova/DL_UCU/tree/master/tasks/hw2\n",
        "    Link to colab: https://colab.research.google.com/drive/1aI3HH3Gj35HHsopXja6bP36Y1heE50Rm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPvCupmXCm71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RXmv73SpT4s",
        "colab_type": "text"
      },
      "source": [
        "## 1. NN training: Torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXkPaLJmN9Ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleConvNet(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(SimpleConvNet, self).__init__()\n",
        "        self.device = device\n",
        "        self.conv_layer = nn.Conv2d(in_channels=1,\n",
        "                                    out_channels=20,\n",
        "                                    kernel_size=5,\n",
        "                                    stride=1,\n",
        "                                    padding=0,\n",
        "                                    dilation=1,\n",
        "                                    groups=1,\n",
        "                                    bias=True)\n",
        "        self.fc_layer1 = nn.Linear(in_features=20 * 12 * 12, out_features=500)\n",
        "        self.fc_layer2 = nn.Linear(in_features=500, out_features=10)\n",
        "        self.to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_conv = self.conv_layer(x)\n",
        "        z_pool = F.max_pool2d(z_conv, 2, 2)\n",
        "        z_pool_reshaped = z_pool.view(-1, 20*12*12)\n",
        "        z_fc1 = self.fc_layer1(z_pool_reshaped)\n",
        "        z_relu = F.relu(z_fc1)\n",
        "        z_fc2 = self.fc_layer2(z_relu)\n",
        "        y = F.softmax(z_fc2, dim=1)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwhssQ5uppRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(torch.log(output), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY0iwZLWU25r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#parameters\n",
        "lr = 0.01\n",
        "batch_size = 64\n",
        "test_batch_size = 64\n",
        "momentum = 0.5\n",
        "\n",
        "no_cuda = False\n",
        "torch.manual_seed(17)\n",
        "np.random.seed(17)\n",
        "epochs = 20\n",
        "log_interval = 10\n",
        "\n",
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=test_batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgExwEztPTPS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a733e9a-597f-45ea-c3df-80cee83b6f9a"
      },
      "source": [
        "#model initialization\n",
        "model = SimpleConvNet(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "#run training\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307000\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.518144\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.992168\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.924957\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.572051\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.573676\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.340825\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.467432\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.496476\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.546279\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.433535\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.286351\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.361834\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.270945\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.485777\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.462483\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.385043\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.282847\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.276706\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.210476\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.243394\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.189055\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.325659\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.290854\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.296236\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.417189\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.194892\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.120417\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.223907\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.265598\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.312217\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.173598\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.096491\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.158035\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.154696\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.171232\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.227243\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.126400\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.073911\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.330485\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.337227\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.173731\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.153728\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.217521\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.298815\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.183423\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.216359\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.248999\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.218063\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.145006\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.322998\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.224265\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.249301\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.101229\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.157044\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.193618\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.360681\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.127120\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.230927\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.256107\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.230529\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.113497\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.337102\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.182331\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.181149\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.106425\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.114616\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.231720\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.235904\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.121349\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.163818\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.190961\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.127562\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.193786\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.363939\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.159351\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.112959\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.121855\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.121905\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.078098\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.177805\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.189057\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.150608\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.205811\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.194158\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.142123\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.290151\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.107158\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.213310\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.117096\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.042760\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.203492\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.113265\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.218708\n",
            "\n",
            "Test set: Average loss: -0.9258, Accuracy: 9604/10000 (96.04%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.046082\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.186428\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.086787\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.227590\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.153162\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.250659\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.070162\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.160104\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.119535\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.088507\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.169920\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.228098\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.174207\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.148103\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.171308\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.176816\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.112068\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.132604\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.087518\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.077219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eFetYreTcgp",
        "colab_type": "text"
      },
      "source": [
        "#### Comment1: Accuracy on MNIST test after training for 20 epoch using pytorch native functions: 98.63% (after runnign 1 epoch - 96.04%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZTg7ed9qjPN",
        "colab_type": "text"
      },
      "source": [
        "----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-8UNt1IqlAT",
        "colab_type": "text"
      },
      "source": [
        "## 2. NN Training: Vector (custom)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJTm3sxQXy4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleConvNet_vector(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(SimpleConvNet_vector, self).__init__()\n",
        "        self.device = device\n",
        "        self.conv_layer = nn.Conv2d(in_channels=1,\n",
        "                                    out_channels=20,\n",
        "                                    kernel_size=5,\n",
        "                                    stride=1,\n",
        "                                    padding=0,\n",
        "                                    dilation=1,\n",
        "                                    groups=1,\n",
        "                                    bias=True)\n",
        "        self.fc_layer1 = nn.Linear(in_features=20 * 12 * 12, out_features=500)\n",
        "        self.fc_layer2 = nn.Linear(in_features=500, out_features=10)\n",
        "        self.to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_conv = conv2d_vector(x, conv_weight=self.conv_layer.weight,\n",
        "                               conv_bias=self.conv_layer.bias,\n",
        "                               device=self.device)\n",
        "        z_pool = pool2d_vector(z_conv, self.device)\n",
        "        z_pool_reshaped = reshape_vector(z_pool, self.device)\n",
        "        z_fc1 = fc_layer_vector(z_pool_reshaped, self.fc_layer1.weight, self.fc_layer1.bias, self.device)\n",
        "        z_relu = relu_vector(z_fc1, self.device)\n",
        "        z_fc2 = fc_layer_vector(z_relu, self.fc_layer2.weight, self.fc_layer2.bias, self.device)\n",
        "        y = F.softmax(z_fc2, dim=1)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBNpUjLmrAgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def im2col(X, kernel_size, device, stride = 1):\n",
        "  \n",
        "    #read dimentions of input tensor - 3-dimentional\n",
        "    C_in, S_in, S_in = X.shape\n",
        "\n",
        "    #calculate size_out\n",
        "    S_out = (S_in - kernel_size)//stride + 1\n",
        "    \n",
        "    #move to device\n",
        "    X = X.to(device)\n",
        "    \n",
        "    #intiale output tensor of the correct size\n",
        "    X_cols = torch.zeros([S_out*S_out, kernel_size*kernel_size]).to(device)\n",
        "    \n",
        "    for i in range(S_out):\n",
        "        for j in range(S_out):\n",
        "            X_cols[i*S_out+j] = X[0][i: i + kernel_size, j: j + kernel_size].contiguous().view(1, -1)\n",
        "    \n",
        "    return X_cols.t() # [K*K x S_out*S_out]\n",
        "\n",
        "  \n",
        "def conv_weight2rows(conv_weight):\n",
        "    \n",
        "    ##read dimentions of input tensor\n",
        "    C_out = conv_weight.shape[0]\n",
        "    kernel_size = conv_weight.shape[2]\n",
        "    \n",
        "    #resize \n",
        "    conv_weight_rows = conv_weight.view(C_out,kernel_size*kernel_size).contiguous()\n",
        "    \n",
        "    return conv_weight_rows # [C_out x K*K]\n",
        "  \n",
        "\n",
        "def conv2d_vector(x_in, conv_weight, conv_bias, device):\n",
        "\n",
        "    #read dimentionas of input tensor and weights\n",
        "    batch_size, C_in, S_in, S_in = x_in.shape\n",
        "    C_out, C_in, kernel_size, kernel_size = conv_weight.shape\n",
        "    \n",
        "    #calculate the dimentions of output tensor\n",
        "    S_out = S_in - kernel_size + 1\n",
        "    \n",
        "    #move to device\n",
        "    x_in = x_in.to(device)\n",
        "    conv_weight = conv_weight.to(device)\n",
        "    conv_bias = conv_bias.to(device)\n",
        "    \n",
        "    #intiale output tensor of the correct size\n",
        "    z = torch.zeros([batch_size,C_out,S_out,S_out]).to(device)\n",
        "    \n",
        "    #transformation of conv_weight\n",
        "    conv_weight_rows = conv_weight2rows(conv_weight)\n",
        "    \n",
        "    for n in range(batch_size):\n",
        "        #WconvX+b, dim(WconvX+b)=[C_out x S_out*S_out], reshape [C_out x S_out x S_out]\n",
        "        z[n] = (conv_weight_rows.matmul(im2col(x_in[n], kernel_size, device, stride=1)) + conv_bias.view(-1,1)).view(C_out,S_out,S_out)\n",
        "    \n",
        "    return z\n",
        "  \n",
        "  \n",
        "  def pool2d_vector(a, device, stride = 2):\n",
        "    \n",
        "    #read dimentionas of input tensor\n",
        "    batch_size, C_in, S_in, S_in = a.shape\n",
        "    pooling_size = 2\n",
        "    stride = 2\n",
        "    \n",
        "    #calculate the dimentions of output tensor\n",
        "    S_out = (S_in - pooling_size)//stride + 1 \n",
        "    C_out = C_in\n",
        "    \n",
        "    #move to device\n",
        "    a = a.to(device)\n",
        "    \n",
        "    #intiale an output tensor of the correct size\n",
        "    z = torch.zeros([batch_size,C_out,S_out,S_out]).to(device)\n",
        "    \n",
        "    for n in range(batch_size):\n",
        "        z[n] = im2col(a[n], pooling_size, device, stride=2).max(dim=0).values.view(-1, S_out, S_out)\n",
        "        \n",
        "    return z \n",
        "  \n",
        "  \n",
        "  def relu_vector(a, device):\n",
        "    \n",
        "    #move to device\n",
        "    a = a.to(device)\n",
        "    \n",
        "    #clone input tensor\n",
        "    z = a.clone().to(device)\n",
        "    \n",
        "    #elements < 0 replace with 0\n",
        "    z[z<0] = 0\n",
        "    \n",
        "    return z\n",
        "  \n",
        "  \n",
        "  def reshape_vector(a, device):\n",
        "    \n",
        "    batch_size = a.shape[0]\n",
        "    \n",
        "    #move to device\n",
        "    a = a.to(device)\n",
        "    \n",
        "    z = a.clone().view(batch_size,-1)\n",
        "    \n",
        "    return z\n",
        "  \n",
        "  \n",
        "  def fc_layer_vector(a, weight, bias, device):\n",
        "    \n",
        "    #move to device\n",
        "    a = a.to(device)\n",
        "    weight = weight.to(device)\n",
        "    bias = bias.to(device)\n",
        "    \n",
        "    z = (a.matmul(weight.t())+ bias).clone()\n",
        "    \n",
        "    return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7F4KYm8O2Dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model initialization\n",
        "model_vector = SimpleConvNet_vector(device)\n",
        "optimizer = optim.SGD(model_vector.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "#run training\n",
        "start_vector = time.time()\n",
        "train(model_vector, device, train_loader, optimizer, epoch=1)\n",
        "end_vector = time.time()\n",
        "print(\"Duration of the 1st epoch: \", (end_vector - start_vector),\"sec\")\n",
        "test(model_vector, device, test_loader)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoHlc6WDtK1H",
        "colab_type": "text"
      },
      "source": [
        "#### Comment2: Accuracy on MNIST test after training for 1 epoch using custom vector functions: 86.47% "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FtN1Y4xtwHm",
        "colab_type": "text"
      },
      "source": [
        "## 3. NN Training: Scalar (training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kxH5uLkt_ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleConvNet_scalar(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(SimpleConvNet_scalar, self).__init__()\n",
        "        self.device = device\n",
        "        self.conv_layer = nn.Conv2d(in_channels=1,\n",
        "                                    out_channels=20,\n",
        "                                    kernel_size=5,\n",
        "                                    stride=1,\n",
        "                                    padding=0,\n",
        "                                    dilation=1,\n",
        "                                    groups=1,\n",
        "                                    bias=True)\n",
        "        self.fc_layer1 = nn.Linear(in_features=20 * 12 * 12, out_features=500)\n",
        "        self.fc_layer2 = nn.Linear(in_features=500, out_features=10)\n",
        "        self.to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_conv = conv2d_scalar(x, conv_weight=self.conv_layer.weight,\n",
        "                               conv_bias=self.conv_layer.bias,\n",
        "                               device=self.device)\n",
        "        z_pool = pool2d_scalar(z_conv, self.device)\n",
        "        z_pool_reshaped = reshape_scalar(z_pool, self.device)\n",
        "        z_fc1 = fc_layer_scalar(z_pool_reshaped, self.fc_layer1.weight, self.fc_layer1.bias, self.device)\n",
        "        z_relu = relu_scalar(z_fc1, self.device)\n",
        "        z_fc2 = fc_layer_scalar(z_relu, self.fc_layer2.weight, self.fc_layer2.bias, self.device)\n",
        "        y = F.softmax(z_fc2, dim=1)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAAbfuxyNu1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d_scalar(x_in, conv_weight, conv_bias, device):\n",
        "    \n",
        "    #read dimentionas of input tensor and weights\n",
        "    batch_size, n_channels_in, height_in, width_in = x_in.shape\n",
        "    n_channels_out, n_channels_in, kernel_size, kernel_size = conv_weight.shape\n",
        "    \n",
        "    #calculate the dimentions of output tensor\n",
        "    height_out = height_in - kernel_size + 1\n",
        "    width_out = width_in - kernel_size + 1\n",
        "    \n",
        "    #move to device\n",
        "    x_in = x_in.to(device)\n",
        "    conv_weight = conv_weight.to(device)\n",
        "    conv_bias = conv_bias.to(device)\n",
        "    \n",
        "    #intiale output tensor of the correct size\n",
        "    z = torch.zeros([batch_size,n_channels_out,height_out,width_out]).to(device)\n",
        "    \n",
        "    #fulfill z based on scalar representation\n",
        "    for n in range(batch_size):\n",
        "        for c_out in range(n_channels_out):\n",
        "            for c_in in range(n_channels_in):\n",
        "                for m in range(height_out):\n",
        "                    for l in range(width_out):\n",
        "                        z[n,c_out,m,l] = (x_in[n,c_in,m:m+kernel_size,l:l+kernel_size]*conv_weight[c_out,c_in]).sum() + conv_bias[c_out]\n",
        "                                                                                                                                                                                                                                                                                                                                                          \n",
        "    return z\n",
        "\n",
        "  \n",
        "def pool2d_scalar(a, device, stride = 2):\n",
        "    \n",
        "    #read dimentionas of input tensor\n",
        "    batch_size, n_channels_in, height_in, width_in = a.shape\n",
        "    pooling_size = 2\n",
        "    \n",
        "    #calculate the dimentions of output tensor\n",
        "    height_out = (height_in-pooling_size)//stride + 1\n",
        "    width_out = (width_in-pooling_size)//stride + 1\n",
        "    n_channels_out = n_channels_in\n",
        "    \n",
        "    #move to device\n",
        "    a = a.to(device)\n",
        "    \n",
        "    #intiale an output tensor of the correct size\n",
        "    z = torch.zeros([batch_size,n_channels_out,height_out,width_out]).to(device)\n",
        "    \n",
        "    #fulfill z based on scalar representation\n",
        "    for n in range(batch_size):\n",
        "        for c_out in range(n_channels_out):\n",
        "            for i in range(height_out):\n",
        "                for j in range(width_out):\n",
        "                    z[n,c_out,i,j] = a[n,c_out,2*i:2*i+2,2*j:2*j+2].max()\n",
        "    \n",
        "    return z\n",
        "\n",
        "  \n",
        "\n",
        "def relu_scalar(a, device):\n",
        "  \n",
        "    #read dimentionas of input matrix\n",
        "    batch_size, n_inputs = a.shape\n",
        "    \n",
        "    #move to device\n",
        "    a = a.to(device)\n",
        "    \n",
        "    #intiale an output matrix of the correct size\n",
        "    z = torch.zeros([batch_size, n_inputs]).to(device)\n",
        "    \n",
        "    for n in range(batch_size):\n",
        "        for i in range(n_inputs):\n",
        "            if a[n,i]<0:\n",
        "                z[n,i]=0\n",
        "            else:\n",
        "                z[n,i]=a[n,i]\n",
        "    #z.requires_grad = True          \n",
        "    \n",
        "    return z\n",
        "\n",
        "  \n",
        "def reshape_scalar(a, device):\n",
        "    \n",
        "    #read dimentionas of input tensor\n",
        "    batch_size, n_channels_in, height_in, width_in = a.shape\n",
        "    \n",
        "    #calculate the dimentions of output tensor\n",
        "    n_outputs = n_channels_in * height_in * width_in\n",
        "    \n",
        "    #move to device\n",
        "    a = a.to(device)\n",
        "    \n",
        "    #intiale an output matrix of the correct size\n",
        "    z = torch.zeros([batch_size, n_outputs]).to(device)\n",
        "    \n",
        "    for n in range(batch_size):\n",
        "        for c_in in range(n_channels_in):\n",
        "            for m in range(height_in):\n",
        "                for l in range(width_in):\n",
        "                    z[n,c_in*height_in*width_in+m*height_in+l] = a[n,c_in,m,l]\n",
        "    \n",
        "    return z\n",
        "  \n",
        "\n",
        "def fc_layer_scalar(a, weight, bias, device):\n",
        "    \n",
        "    #read dimentionas of input matrix\n",
        "    batch_size, n_inputs = a.shape\n",
        "    n_outputs = bias.shape[0]\n",
        "    \n",
        "    #move to device\n",
        "    a = a.to(device)\n",
        "    weight = weight.to(device)\n",
        "    bias = bias.to(device)\n",
        "    \n",
        "    #intiale an output matrix of the correct size\n",
        "    z = torch.zeros([batch_size, n_outputs]).to(device)\n",
        "    \n",
        "    for n in range(batch_size):\n",
        "        for j in range(n_outputs):\n",
        "            z[n,j] = bias[j]\n",
        "            for i in range(n_inputs):\n",
        "                z[n,j] += weight[j,i]*a[n,i]\n",
        "                \n",
        "    return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVIoduAqY0-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model initialization\n",
        "model_scalar = SimpleConvNet_scalar(device)\n",
        "optimizer = optim.SGD(model_scalar.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "#run training\n",
        "start_scalar = time.time()\n",
        "train(model_scalar, device, train_loader, optimizer, epoch = 1)\n",
        "end_scalar = time.time()\n",
        "print(\"Duration of the 1st epoch: \", (end_scalar - start_scalar),\"sec\")\n",
        "test(model_vector, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVx0kf02Fgw3",
        "colab_type": "text"
      },
      "source": [
        "#### Comment3. Scalar functions are veeeeeery slow.. Please find the performance comparison run on dummy data here: https://colab.research.google.com/drive/1TMqoh8WRat9IsbrVJlDdMCfiuG6bxa90"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m-XsV4EGA15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}